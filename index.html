<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Learning Hierarchical Orthogonal Prototypes for Generalized Few-Shot 3D Point Cloud Segmentation - Anonymous ICME submission">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="HOP3D learns hierarchical orthogonal prototypes with an entropy-based few-shot regularizer for generalized few-shot 3D point cloud segmentation, improving novel adaptation without base forgetting.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="generalized few-shot learning, 3D point cloud segmentation, hierarchical prototypes, orthogonal regularization, prototype learning, entropy regularization, ScanNet200, ScanNet++">
  <!-- TODO: List all authors -->
  <meta name="author" content="Anonymous ICME submission">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="HOP3D Project Page">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Learning Hierarchical Orthogonal Prototypes for Generalized Few-Shot 3D Point Cloud Segmentation">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="HOP3D learns hierarchical orthogonal prototypes with an entropy-based few-shot regularizer for generalized few-shot 3D point cloud segmentation, improving novel adaptation without base forgetting.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="./">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="HOP3D - Research Preview">
  <meta property="article:published_time" content="2026-02-05T00:00:00.000Z">
  <meta property="article:author" content="Anonymous ICME submission">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="generalized few-shot learning">
  <meta property="article:tag" content="3D point cloud segmentation">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="Learning Hierarchical Orthogonal Prototypes for Generalized Few-Shot 3D Point Cloud Segmentation">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="HOP3D learns hierarchical orthogonal prototypes with an entropy-based few-shot regularizer for generalized few-shot 3D point cloud segmentation, improving novel adaptation without base forgetting.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="static/images/social_preview.png">
  <meta name="twitter:image:alt" content="HOP3D - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Learning Hierarchical Orthogonal Prototypes for Generalized Few-Shot 3D Point Cloud Segmentation">
  <meta name="citation_author" content="Anonymous ICME submission">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="ICME (under review)">
  <meta name="citation_pdf_url" content="static/pdfs/ICME_2026_Paper_HOP3D.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>HOP3D | Learning Hierarchical Orthogonal Prototypes for GFS 3D Segmentation</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Learning Hierarchical Orthogonal Prototypes for Generalized Few-Shot 3D Point Cloud Segmentation",
    "description": "HOP3D learns hierarchical orthogonal prototypes with an entropy-based few-shot regularizer for generalized few-shot 3D point cloud segmentation, improving novel adaptation without base forgetting.",
    "author": [
      {
        "@type": "Person",
        "name": "Anonymous ICME submission",
        "affiliation": {
          "@type": "Organization",
          "name": "Anonymous"
        }
      }
    ],
    "datePublished": "2026-02-05",
    "publisher": {
      "@type": "Organization",
      "name": "ICME (under review)"
    },
    "url": "./",
    "image": "static/images/social_preview.png",
    "keywords": ["generalized few-shot learning", "3D point cloud segmentation", "orthogonal regularization", "prototype learning", "entropy regularization"],
    "abstract": "Generalized few-shot 3D point cloud segmentation aims to adapt to novel classes from only a few annotations while maintaining strong performance on base classes, but this remains challenging due to the stability–plasticity trade-off. We present HOP3D, a unified framework that learns hierarchical orthogonal prototypes with an entropy-based few-shot regularizer to enable robust novel-class adaptation without degrading base-class performance. HOP3D introduces hierarchical orthogonalization that decouples base and novel learning at both the gradient and representation levels, effectively mitigating base–novel interference. To further enhance adaptation under sparse supervision, we incorporate an entropy-based regularizer that leverages predictive uncertainty to refine prototype learning and promote balanced predictions. Experiments on ScanNet200 and ScanNet++ show that HOP3D consistently outperforms prior methods under both 1-shot and 5-shot settings.",
    "citation": "@inproceedings{HOP3D2026,\n  title={Learning Hierarchical Orthogonal Prototypes for Generalized Few-Shot 3D Point Cloud Segmentation},\n  author={Anonymous},\n  booktitle={IEEE International Conference on Multimedia \\& Expo (ICME)},\n  year={2026},\n  note={Under review}\n}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "./"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Generalized Few-Shot Learning"
      },
      {
        "@type": "Thing", 
        "name": "3D Point Cloud Segmentation"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "./",
    "logo": "static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown (optional)
       Disabled by default to avoid template placeholders on the final page.
       Re-enable once you have real related works links.
  -->
  <!--
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="#" class="work-item" aria-disabled="true">
          <div class="work-info">
            <h5>Coming soon</h5>
            <p>Add your lab's related works here.</p>
            <span class="work-venue">—</span>
          </div>
        </a>
      </div>
    </div>
  </div>
  -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">Learning Hierarchical Orthogonal Prototypes for Generalized Few-Shot 3D Point Cloud Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <span>Anonymous ICME submission</span>
              </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block">ICME (Under Review) · 2026</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Replace with your final paper PDF (local file recommended) -->
                      <span class="link-block">
                        <a href="static/pdfs/ICME_2026_Paper_HOP3D.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <span class="link-block">
                      <a href="static/pdfs/HOP3D_SM.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Code repository -->
                  <span class="link-block">
                    <a href="https://github.com/fdueblab-HOP3D/HOP3D?tab=readme-ov-file" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Optional: arXiv link (enable once available) -->
                <!--
                <span class="link-block">
                  <a href="https://arxiv.org/abs/ARXIV_PAPER_ID" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
                </span>
                -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light" id="abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Generalized few-shot 3D point cloud segmentation aims to adapt to novel classes from only a few annotations while maintaining strong performance on base classes, but this remains challenging due to the inherent stability--plasticity trade-off: adapting to novel classes can interfere with shared representations and cause base-class forgetting.
            We present HOP3D, a unified framework that learns hierarchical orthogonal prototypes with an entropy-based few-shot regularizer to enable robust novel-class adaptation without degrading base-class performance.
            HOP3D introduces hierarchical orthogonalization that decouples base and novel learning at both the gradient and representation levels, effectively mitigating base--novel interference.
            To further enhance adaptation under sparse supervision, we incorporate an entropy-based regularizer that leverages predictive uncertainty to refine prototype learning and promote balanced predictions.
            Extensive experiments on ScanNet200 and ScanNet++ demonstrate that HOP3D consistently outperforms state-of-the-art baselines under both 1-shot and 5-shot settings. The code will be publicly released upon acceptance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Method -->
<section class="section" id="method">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths content has-text-justified">
        <p>
          We propose <strong>HOP3D</strong>, a unified framework for generalized few-shot 3D point cloud segmentation that reduces base–novel interference and improves few-shot adaptation robustness.
          HOP3D integrates <strong>HOP-Net</strong> (hierarchical orthogonalization) and <strong>HOP-Ent</strong> (entropy-based few-shot regularizer).
        </p>
        <ul>
          <li><strong>HOP-Grad</strong>: projects Phase-2 novel gradients onto the orthogonal complement of base-task gradient directions to mitigate base forgetting.</li>
          <li><strong>HOP-Rep</strong>: learns orthogonal prototype subspaces for a base/novel representation decomposition and more stable decision geometry.</li>
          <li><strong>HOP-Ent</strong>: dual-entropy regularization to encourage confident and balanced novel predictions under sparse supervision.</li>
        </ul>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <figure class="figure-block">
          <img src="static/images/framework.png" alt="Overview of the HOP3D framework (HOP-Net + HOP-Ent)" loading="eager"/>
          <figcaption class="figure-caption">
            Overview of HOP3D: two-phase training with hierarchical orthogonalization (HOP-Grad + HOP-Rep) and entropy-guided refinement (HOP-Ent).
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>
<!-- End method -->

<!-- Qualitative Results -->
<section class="section hero is-light" id="qualitative-results">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results</h2>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <figure class="figure-block">
          <img src="static/images/main_vis.png" alt="Qualitative comparison between GFS-VL and HOP3D on ScanNet200" loading="lazy"/>
          <figcaption class="figure-caption">
            Qualitative comparison on ScanNet200: compared with GFS-VL, HOP3D produces more consistent base/novel predictions and reduces typical confusions.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>
<!-- End qualitative results -->

<!-- Experimental Results -->
<section class="section" id="experimental-results">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths content">
        <p class="has-text-centered is-size-6 has-text-grey">
          Metrics: mean IoU on base classes (B), novel classes (N), all classes (A), and their harmonic mean (HM). Results for 1-shot and 5-shot on ScanNet200 / ScanNet++.
        </p>

        <h3 class="title is-5">ScanNet200</h3>
        <div class="table-container results-table results-table-split">
          <table class="table is-striped is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th rowspan="2">Method</th>
                <th colspan="4">5-shot</th>
                <th colspan="4">1-shot</th>
              </tr>
              <tr>
                <th>B</th><th>N</th><th>A</th><th>HM</th>
                <th>B</th><th>N</th><th>A</th><th>HM</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Fully Sup.</td>
                <td>68.70</td><td>39.32</td><td>45.51</td><td>50.02</td>
                <td>68.70</td><td>39.32</td><td>45.51</td><td>50.02</td>
              </tr>
              <tr>
                <td>PIFS</td>
                <td>28.78</td><td>3.82</td><td>9.07</td><td>6.71</td>
                <td>17.84</td><td>2.87</td><td>6.02</td><td>4.88</td>
              </tr>
              <tr>
                <td>attMPTI</td>
                <td>37.13</td><td>4.99</td><td>11.76</td><td>8.79</td>
                <td>54.84</td><td>3.28</td><td>14.14</td><td>6.17</td>
              </tr>
              <tr>
                <td>COSeg</td>
                <td>57.67</td><td>5.21</td><td>16.25</td><td>9.54</td>
                <td>47.03</td><td>4.03</td><td>13.09</td><td>7.42</td>
              </tr>
              <tr>
                <td>GW</td>
                <td>59.28</td><td>8.30</td><td>19.03</td><td>14.55</td>
                <td>55.23</td><td>6.47</td><td>16.74</td><td>11.56</td>
              </tr>
              <tr>
                <td>GFS-VL</td>
                <td>67.17</td><td>31.18</td><td>38.76</td><td>42.59</td>
                <td>67.25</td><td>28.89</td><td>36.97</td><td>40.42</td>
              </tr>
              <tr class="is-selected">
                <td><strong>HOP3D (ours)</strong></td>
                <td><strong>67.36</strong></td><td><strong>34.38</strong></td><td><strong>41.32</strong></td><td><strong>45.52</strong></td>
                <td><strong>68.45</strong></td><td><strong>31.80</strong></td><td><strong>39.52</strong></td><td><strong>43.42</strong></td>
              </tr>
            </tbody>
          </table>
        </div>

        <h3 class="title is-5">ScanNet++</h3>
        <div class="table-container results-table results-table-split">
          <table class="table is-striped is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th rowspan="2">Method</th>
                <th colspan="4">5-shot</th>
                <th colspan="4">1-shot</th>
              </tr>
              <tr>
                <th>B</th><th>N</th><th>A</th><th>HM</th>
                <th>B</th><th>N</th><th>A</th><th>HM</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Fully Sup.</td>
                <td>65.45</td><td>37.24</td><td>48.53</td><td>47.47</td>
                <td>65.45</td><td>37.24</td><td>48.53</td><td>47.47</td>
              </tr>
              <tr>
                <td>PIFS</td>
                <td>39.98</td><td>5.74</td><td>19.44</td><td>10.03</td>
                <td>36.66</td><td>4.95</td><td>17.63</td><td>8.71</td>
              </tr>
              <tr>
                <td>attMPTI</td>
                <td>55.89</td><td>4.19</td><td>24.87</td><td>7.78</td>
                <td>53.16</td><td>3.55</td><td>23.40</td><td>6.66</td>
              </tr>
              <tr>
                <td>COSeg</td>
                <td>59.34</td><td>6.96</td><td>27.91</td><td>12.45</td>
                <td>58.49</td><td>6.24</td><td>27.14</td><td>11.26</td>
              </tr>
              <tr>
                <td>GW</td>
                <td>51.35</td><td>11.03</td><td>27.16</td><td>18.15</td>
                <td>46.71</td><td>6.63</td><td>22.66</td><td>11.59</td>
              </tr>
              <tr>
                <td>GFS-VL</td>
                <td>60.49</td><td>21.40</td><td>37.04</td><td>31.61</td>
                <td>60.02</td><td>17.90</td><td>34.75</td><td>27.56</td>
              </tr>
              <tr class="is-selected">
                <td><strong>HOP3D (ours)</strong></td>
                <td><strong>62.40</strong></td><td><strong>23.70</strong></td><td><strong>39.18</strong></td><td><strong>34.34</strong></td>
                <td><strong>61.72</strong></td><td><strong>19.23</strong></td><td><strong>36.23</strong></td><td><strong>29.32</strong></td>
              </tr>
            </tbody>
          </table>
        </div>

        <h3 class="title is-5">Key Findings</h3>
        <ul>
          <li><strong>Consistent novel gains</strong>: HOP3D improves mIoU-N and HM over the strongest baseline (GFS-VL) on both datasets under 1-shot and 5-shot.</li>
          <li><strong>Base retention</strong>: HOP3D maintains strong base-class performance while improving novel recognition.</li>
          <li><strong>Entropy regularization helps</strong>: HOP-Ent improves confidence and class balance during Phase-2 adaptation.</li>
        </ul>
      </div>
    </div>
  </div>
</section>
<!-- End experimental results -->

<!-- Ablation / Additional Analysis -->
<section class="section hero is-light" id="analysis">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Ablation & Additional Analysis</h2>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <figure class="figure-block analysis-figure analysis-wide">
          <img src="static/images/fig_orthogonal_1x4_ieee.png" alt="Prototype cosine similarity matrices (1x4)" loading="lazy"/>
          <figcaption class="figure-caption">
            Prototype cosine-similarity matrices of ℓ2-normalized prototypes (Phase 1/2, with/without HOP-Net).
          </figcaption>
        </figure>

        <div class="columns is-variable is-4 analysis-row">
          <div class="column is-5">
            <figure class="figure-block analysis-figure analysis-portrait">
              <img src="static/images/combined_entropy_analysis_2x1.png" alt="HOP-Ent analysis (confidence and class balance)" loading="lazy"/>
              <figcaption class="figure-caption">
                HOP-Ent analysis: confidence distribution and class-frequency distribution (2×1).
              </figcaption>
            </figure>
          </div>
          <div class="column is-7">
            <figure class="figure-block analysis-figure">
              <img src="static/images/fig_hog_ablation.png" alt="HOP-Net ablation (lambda_orth and adaptation ratio)" loading="lazy"/>
              <figcaption class="figure-caption">
                HOP-Net ablation: impact of orthogonality weight (λ<sub>orth</sub>) and adaptation ratio (AR).
              </figcaption>
            </figure>
          </div>
        </div>

        <figure class="figure-block analysis-figure analysis-wide">
          <img src="static/images/ablation_vis.png" alt="Qualitative ablation results" loading="lazy"/>
          <figcaption class="figure-caption">
            Qualitative ablation: HOP3D corrects typical base–novel confusions compared with variants without HOP-Net/HOP-Ent.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>
<!-- End analysis -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{HOP3D2026,
  title={Learning Hierarchical Orthogonal Prototypes for Generalized Few-Shot 3D Point Cloud Segmentation},
  author={Anonymous},
  booktitle={IEEE International Conference on Multimedia \&amp; Expo (ICME)},
  year={2026},
  note={Under review},
  url={https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
